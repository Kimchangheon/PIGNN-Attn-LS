{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### MLP as Aggregator\n",
    "\n",
    "At each **message** passing step $k$, the generic form is:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "m_i^{(k+1)}\n",
    "&= \\mathrm{UPDATE}^{(k)}\\!\\Big(\n",
    "m_i^{(k)},\\,\n",
    "\\Phi_{\\mathrm{phys}}^{(k)}\\big(\n",
    "\\mathrm{AGGREGATE}^{(k)}(\\{\\,m_j^{(k)} \\mid j \\in \\mathcal{N}(i)\\,\\}),\\,\n",
    "\\mathrm{state}^{(k)}\n",
    "\\big)\\Big).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The detailed steps are as follows.\n",
    "\n",
    "1. **M1 — Message aggregation (degree-normalized).**\n",
    "   For each undirected edge $j \\to i$ with line attributes\n",
    "   $\\ell_{ij}=(g_{ij},\\,b_{ij},\\,b_{ij}^{\\mathrm{sh}})$ (series conductance, susceptance, end-shunt),\n",
    "\n",
    "    $$\n",
    "    \\phi_{ij}^{(k)} \\;=\\; \\mathrm{MLP}_{\\phi}^{(k)}\\!\\big(m_j^{(k)},\\,\\ell_{ij}\\big),\n",
    "    \\qquad\n",
    "    M_i^{(k)} \\;=\\; \\frac{1}{\\lvert \\mathcal N(i)\\rvert} \\sum_{j\\in\\mathcal N(i)} \\phi_{ij}^{(k)}.\n",
    "    $$\n",
    "\n",
    "2. **M2 — Physics-informed transformation $\\Phi_{\\mathrm{phys}}^{(k)}$.**\n",
    "   Given $Y$ (built from line attributes per the problem formulation), compute\n",
    "\n",
    "    $$\n",
    "    \\Delta P_i^{(k)} = P_i^{\\mathrm{set}} - P_i^{(k)}, \\qquad\n",
    "    \\Delta Q_i^{(k)} = Q_i^{\\mathrm{set}} - Q_i^{(k)}.\n",
    "    $$\n",
    "\n",
    "   Masks enforce $\\Delta P_i{=}\\Delta Q_i{=}0$ on **slack** buses and $\\Delta Q_i{=}0$ on **PV** buses.\n",
    "\n",
    "3. **M3 — Node update.**\n",
    "   Concatenate electrical and learned features,\n",
    "\n",
    "    $$\n",
    "    \\mathrm{ctx}_i^{(k)}\n",
    "    = \\big[V_i^{(k)},\\,\\theta_i^{(k)},\\,\\Delta P_i^{(k)},\\,\\Delta Q_i^{(k)},\\,m_i^{(k)},\\,M_i^{(k)}\\big]\n",
    "    \\in \\mathbb R^{4+2d},\n",
    "    $$\n",
    "\n",
    "   predict increments,\n",
    "\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "      \\Delta\\theta_i^{(k)} &= L_\\theta^{(k)}\\!\\big(\\mathrm{ctx}_i^{(k)}\\big), \\\\\n",
    "      \\Delta V_i^{(k)}     &= L_v^{(k)}\\!\\big(\\mathrm{ctx}_i^{(k)}\\big), \\\\\n",
    "      \\Delta m_i^{(k)}     &= \\tanh\\!\\Big(L_m^{(k)}\\!\\big(\\mathrm{ctx}_i^{(k)}\\big)\\Big),\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "\n",
    "   and apply masked updates:\n",
    "\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "      \\theta_i^{(k+1)} &= \\theta_i^{(k)} + \\Delta\\theta_i^{(k)}, \\\\\n",
    "      V_i^{(k+1)}      &= V_i^{(k)}      + \\Delta V_i^{(k)}, \\\\\n",
    "      m_i^{(k+1)}      &= m_i^{(k)}      + \\Delta m_i^{(k)}.\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "\n",
    "4. **M4 — Physics-informed loss (discounted).**\n",
    "\n",
    "    $$\n",
    "    \\mathcal L_{\\rm phys}\n",
    "    \\;=\\; \\sum_{k=0}^{K-1}\n",
    "      \\gamma^{\\,K-1-k}\\,\n",
    "      \\frac{1}{N}\\sum_{i=1}^N \\Big[(\\Delta P_i^{(k)})^2+(\\Delta Q_i^{(k)})^2\\Big],\n",
    "    \\qquad \\gamma\\in(0,1].\n",
    "    $$\n",
    "---\n",
    "\n",
    "### Self-Attention as Aggregator\n",
    "\n",
    "For the attention variant, compute the physics features as in **M2** and then replace **M1** by an edge-conditioned self-attention that is sparse and state-dependent.\n",
    "\n",
    "1. **A1 — Node embedding.** At step $k$,\n",
    "\n",
    "    $$\n",
    "    b_i^{(k)}=\\big[V_i^{(k)},\\,\\theta_i^{(k)},\\,\\Delta P_i^{(k)},\\,\\Delta Q_i^{(k)},\\,m_i^{(k)}\\big]\\in\\mathbb{R}^{4+d}.\n",
    "    $$\n",
    "\n",
    "2. **A2 — Edge-wise multi-head attention ($O(EH)$).**\n",
    "   For each undirected edge $\\{i,j\\}$, score both directions $j{\\to}i$ and $i{\\to}j$.\n",
    "   For head $h=1,\\dots,H$ with $d_h=d_{\\mathrm{model}}/H$,\n",
    "\n",
    "    $$\n",
    "    q_i^{(h)}=W_Q^{(h)} b_i^{(k)},\\quad\n",
    "    k_j^{(h)}=W_K^{(h)} b_j^{(k)},\\quad\n",
    "    u_j^{(h)}=W_V^{(h)} b_j^{(k)}.\n",
    "    $$\n",
    "\n",
    "   Add a physics-based edge bias from $\\ell_{ij}$,\n",
    "\n",
    "    $$\n",
    "    s_{ij}^{(h)}=\\frac{\\langle q_i^{(h)},k_j^{(h)}\\rangle}{\\sqrt{d_h}}+\\beta_{ij}^{(h)},\n",
    "    \\qquad\n",
    "    \\beta_{ij}^{(h)}=f_{\\mathrm{edge}}^{(h)}(\\ell_{ij}),\n",
    "    $$\n",
    "\n",
    "   and normalize over incoming neighbors of $i$:\n",
    "\n",
    "    $$\n",
    "    \\alpha_{ij}^{(h)}=\\frac{\\exp(s_{ij}^{(h)})}{\\sum_{u\\in\\mathcal N(i)}\\exp(s_{iu}^{(h)})}.\n",
    "    $$\n",
    "\n",
    "3. **A3 — Attention context (supplants raw concatenation).**\n",
    "\n",
    "    $$\n",
    "    \\mathrm{ctx}_i^{(k)}\n",
    "    = W_O\\!\\left[\n",
    "    \\sum_{j}\\alpha_{ij}^{(1)}u_j^{(1)}\\;\\big\\|\\;\\cdots\\;\\big\\|\\;\\sum_{j}\\alpha_{ij}^{(H)}u_j^{(H)}\n",
    "    \\right]\\in\\mathbb{R}^{d}.\n",
    "    $$\n",
    "\n",
    "   This replaces the entire concatenated feature: the **M3** update heads now take $\\mathrm{ctx}_i^{(k)}$ alone, since $(V,\\theta,\\Delta P,\\Delta Q,m)$ already condition the attention via $(q,k,v)$.\n",
    "\n",
    "> **Notes.** Attention operates only on existing edges (no dense $N^2$ attention) with a per-destination softmax; when logits are uniform it reduces to a degree-normalized neighbor average."
   ],
   "id": "9c236b4a7191313a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
